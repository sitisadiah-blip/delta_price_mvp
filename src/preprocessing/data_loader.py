"""
å¤šæ•°æ®é›†åŠ è½½å™¨
æ”¯æŒåŠ è½½ train.xlsx å’Œ data/zx/ ç›®å½•ä¸‹çš„å¤šä¸ªæ•°æ®æ–‡ä»¶
"""

import os
import glob
import pandas as pd
from typing import List, Tuple
import warnings

warnings.filterwarnings('ignore')


class MultiDatasetLoader:
    """å¤šæ•°æ®é›†åŠ è½½ä¸åˆå¹¶"""
    
    def __init__(self, base_dir: str = "."):
        """
        åˆå§‹åŒ–åŠ è½½å™¨
        
        Args:
            base_dir: é¡¹ç›®æ ¹ç›®å½•
        """
        self.base_dir = base_dir
        self.original_data = None
        self.zx_data = None
        self.combined_data = None
    
    def load_original_data(self, filepath: str = "data/train.xlsx") -> pd.DataFrame:
        """
        åŠ è½½åŸå§‹æ•°æ®
        
        Args:
            filepath: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            pd.DataFrame: åŸå§‹æ•°æ®
        """
        full_path = os.path.join(self.base_dir, filepath)
        if not os.path.exists(full_path):
            raise FileNotFoundError(f"åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
        
        print(f"ğŸ“– åŠ è½½åŸå§‹æ•°æ®: {filepath}")
        self.original_data = pd.read_excel(full_path)
        print(f"   âœ“ å½¢çŠ¶: {self.original_data.shape}")
        return self.original_data
    
    def load_zx_datasets(self, zx_dir: str = "data/zx") -> pd.DataFrame:
        """
        åŠ è½½å¹¶åˆå¹¶ zx ç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶
        
        Args:
            zx_dir: zx æ•°æ®ç›®å½•
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„ zx æ•°æ®
        """
        full_path = os.path.join(self.base_dir, zx_dir)
        if not os.path.exists(full_path):
            raise FileNotFoundError(f"zx æ•°æ®ç›®å½•ä¸å­˜åœ¨: {full_path}")
        
        # æŸ¥æ‰¾æ‰€æœ‰ xlsx æ–‡ä»¶
        xlsx_files = glob.glob(os.path.join(full_path, "*.xlsx"))
        print(f"ğŸ“‚ å‘ç° {len(xlsx_files)} ä¸ª zx æ•°æ®æ–‡ä»¶")
        
        dfs = []
        for file_path in sorted(xlsx_files):
            filename = os.path.basename(file_path)
            try:
                df = pd.read_excel(file_path)
                dfs.append(df)
                print(f"   âœ“ åŠ è½½ {filename}: {df.shape[0]} è¡Œ")
            except Exception as e:
                print(f"   âœ— åŠ è½½å¤±è´¥ {filename}: {e}")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if dfs:
            self.zx_data = pd.concat(dfs, ignore_index=True)
            print(f"\nâœ“ zx æ•°æ®åˆå¹¶å®Œæˆ: {self.zx_data.shape}")
        else:
            raise ValueError("æœªæˆåŠŸåŠ è½½ä»»ä½• zx æ•°æ®æ–‡ä»¶")
        
        return self.zx_data
    
    def get_common_columns(self) -> List[str]:
        """è·å–ä¸¤ä¸ªæ•°æ®é›†çš„å…¬å…±åˆ—"""
        if self.original_data is None or self.zx_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½åŸå§‹æ•°æ®å’Œ zx æ•°æ®")
        
        common = list(set(self.original_data.columns) & set(self.zx_data.columns))
        print(f"\nğŸ“Š å…¬å…±åˆ—æ•°: {len(common)}")
        if common:
            print(f"   {common}")
        return common
    
    def combine_datasets(self, on_columns: List[str] = None) -> pd.DataFrame:
        """
        åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†
        
        Args:
            on_columns: åˆå¹¶æ—¶ä½¿ç”¨çš„åˆ—ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨ concatï¼‰
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„æ•°æ®
        """
        if self.original_data is None or self.zx_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½åŸå§‹æ•°æ®å’Œ zx æ•°æ®")
        
        print("\nğŸ”— å¼€å§‹åˆå¹¶æ•°æ®é›†...")
        
        # ç®€å•çš„è¡Œçº§åˆ«åˆå¹¶ï¼ˆappendï¼‰
        # ä½¿ç”¨å…±åŒåˆ—å’Œå¡«å……
        common_cols = self.get_common_columns()
        
        # æ ‡è®°æ•°æ®æ¥æº
        self.original_data['data_source'] = 'original'
        self.zx_data['data_source'] = 'zx'
        
        # å¯¹é½åˆ—
        all_cols = sorted(set(list(self.original_data.columns) + list(self.zx_data.columns)))
        
        # å¡«å……ç¼ºå¤±åˆ—
        for col in all_cols:
            if col not in self.original_data.columns:
                self.original_data[col] = None
            if col not in self.zx_data.columns:
                self.zx_data[col] = None
        
        # åˆå¹¶
        self.combined_data = pd.concat(
            [self.original_data[all_cols], self.zx_data[all_cols]],
            ignore_index=True,
            sort=False
        )
        
        print(f"âœ“ åˆå¹¶å®Œæˆ: {self.combined_data.shape}")
        print(f"  åŸå§‹æ•°æ®: {len(self.original_data)} è¡Œ")
        print(f"  zx æ•°æ®: {len(self.zx_data)} è¡Œ")
        print(f"  åˆå¹¶ç»“æœ: {len(self.combined_data)} è¡Œ, {len(self.combined_data.columns)} åˆ—")
        
        return self.combined_data
    
    def get_combined_data(self) -> pd.DataFrame:
        """è·å–åˆå¹¶åçš„æ•°æ®"""
        if self.combined_data is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ combine_datasets()")
        return self.combined_data


if __name__ == '__main__':
    # æµ‹è¯•è„šæœ¬
    loader = MultiDatasetLoader(base_dir='.')
    
    print("=" * 80)
    print("ğŸš€ å¤šæ•°æ®é›†åŠ è½½ä¸åˆå¹¶æµ‹è¯•")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    loader.load_original_data()
    loader.load_zx_datasets()
    
    # åˆå¹¶æ•°æ®
    combined = loader.combine_datasets()
    
    print("\nğŸ“Š åˆå¹¶åçš„æ•°æ®é¢„è§ˆ:")
    print(combined.head())
    
    print("\nâœ“ æ•°æ®æ¥æºåˆ†å¸ƒ:")
    print(combined['data_source'].value_counts())
